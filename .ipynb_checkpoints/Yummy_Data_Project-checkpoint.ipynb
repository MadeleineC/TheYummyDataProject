{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I\n",
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "%matplotlib inline\n",
    "import logging\n",
    "from time import sleep\n",
    "import datetime\n",
    "import requests\n",
    "import numpy as np\n",
    "import re\n",
    "from xml.etree import ElementTree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II  Function get_zipcode\n",
    "** please note\n",
    "Take input of city and state from a customer\n",
    "Make API request to 'www.webservicex.net/uszip.asmx/GetInfoByCity'\n",
    "return a zipcode of list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_zipcode(city,state):\n",
    "    city_df = pd.DataFrame({\n",
    "    \"City\" : \"\",\n",
    "    \"State\" : \"\",\n",
    "    \"Zipcode\" : \"\",\n",
    "    \"Area_Code\" : \"\",\n",
    "    \"Time_Zone\" : \"\"\n",
    "    }, \n",
    "    index=[0])\n",
    "\n",
    "    # root_url='www.webservicex.net/uszip.asmx/GetInfoByCity?USCity=%s HTTP/1.1' % (city)\n",
    "    root_url='http://www.webservicex.net/uszip.asmx/GetInfoByCity?USCity=%s' % (city)\n",
    "    initial_url=root_url\n",
    "    response=requests.get(initial_url)\n",
    "    response = requests.get(initial_url, stream=True)\n",
    "    response.raw.decode_content = True\n",
    "    with open('data.xml', 'w') as f:\n",
    "        f.write(response.text)\n",
    "    tree = ElementTree.parse('data.xml')\n",
    "    root = tree.getroot()\n",
    "    zip_list = []\n",
    "    index = 0\n",
    "\n",
    "\n",
    "    for i in range(0,len(root)):\n",
    "        for x in range(0,len(root[i])):\n",
    "    #         print(root[i][x].text)\n",
    "            if (root[i][x].text) == state:\n",
    "                if (root[i][0].text) == city:\n",
    "    #                 print(len(zip_list))\n",
    "                    zip_list.append(root[i][2].text)\n",
    "            if x == 0:\n",
    "                city_df.set_value(index,[\"City\"],root[i][x].text)\n",
    "            elif x == 1:\n",
    "                \n",
    "                city_df.set_value(index,[\"State\"],root[i][x].text)\n",
    "            elif x == 2:\n",
    "                city_df.set_value(index,[\"Zipcode\"],root[i][x].text)\n",
    "            elif x == 3:\n",
    "                city_df.set_value(index,[\"Area_Code\"],root[i][x].text)\n",
    "            elif x == 4:\n",
    "                city_df.set_value(index,[\"Time_Zone\"],root[i][x].text)\n",
    "        index = index + 1\n",
    "    # print(len(zip_list))\n",
    "    # print(zip_list)\n",
    "    return zip_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III Function get restaurant info from google place API\n",
    "Input: zip_code\n",
    "       key_word to search\n",
    "       city\n",
    "       state\n",
    "\n",
    "search method: 'https://maps.googleapis.com/maps/api/place/textsearch/json?'\n",
    "                with key_word('pizza'), type('restaurant') and zip_code\n",
    "\n",
    "output: 1.search results in output_key_word_in_city_state/google_place_API_zip_code_city_state.txt\n",
    "        2.info_list with all results of all zipcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_google_place(key_word,zip_list,city,state):\n",
    "    #google places API \n",
    "    api_key='AIzaSyB_i6T2QOcNmKeJUxF6FC5B1XN1j-mixjI'\n",
    "\n",
    "#     key_word='chinese'\n",
    "    search_type='restaurant'\n",
    "    root_url='https://maps.googleapis.com/maps/api/place/textsearch/json?'\n",
    "    try:\n",
    "        output_folder=os.makedirs('output_{}_in_{}_{}'.format(key_word,city,state))\n",
    "    except OSError as e:\n",
    "        output_folder='output_{}_in_{}_{}'.format(key_word,city,state)\n",
    "        \n",
    "    info_list=[]\n",
    "    #loop in zip_code list\n",
    "    for i in zip_list:\n",
    "        #set up a page counter\n",
    "        page=1\n",
    "        # logging.info(page)\n",
    "        #set up url for the first call \n",
    "        initial_url=root_url+'query={}+in+{}'.format(key_word,i)+'&type='+search_type+'&key='+api_key\n",
    "        response=requests.get(initial_url).json()\n",
    "        \n",
    "        #extract results in a variable called results\n",
    "        results=response['results']\n",
    "        \n",
    "        logging.info('zip_code:{} page:{}/n {}/n/n'.format(i,page,initial_url))\n",
    "        logging.info('debug hey i have {} restaurants'.format(len(results)))\n",
    "        \n",
    "        #assign a variable to store nextpage token\n",
    "        next_page=response.get(\"next_page_token\")\n",
    "        \n",
    "        #sometimes it give me empty results if I make calls too often \n",
    "        sleep(2)\n",
    "        \n",
    "        #use a while loop for flipping pages, google allows a maximum of 3 pages \n",
    "        while bool(next_page)==True: \n",
    "            logging.info('debug hey I am under the while loop')\n",
    "            #update page counter\n",
    "            page+=1\n",
    "#             logging.info('debug the current page is {}'.format(page))\n",
    "            #set up API call for the next page and logging it\n",
    "            next_page_url=root_url+\"pagetoken=\"+next_page+'&key='+api_key\n",
    "            logging.info('zip_code:{} page:{}/n {}/n/n'.format(i,page,next_page_url))\n",
    "            response=requests.get(next_page_url).json()\n",
    "            logging.info('debug current page has {} restaurants'.format(len(response['results'])))\n",
    "            \n",
    "            #Add the new results to results here using + NOT append because:\n",
    "            #list_1=[1,2,3] list_2=[4,5,6] list_1+list_2=[1,2,3,4,5,6]\n",
    "            #list_1.append(list2)------[1,2,3,[4,5,6]]\n",
    "            results=results+response[\"results\"]\n",
    "            logging.info('debug now i have a total of {} restaurants'.format(len(results)))\n",
    "            sleep(2)\n",
    "            #update next_page token\n",
    "            next_page=response.get(\"next_page_token\")\n",
    "            logging.info('debug the current token is {}'.format(next_page))\n",
    "        \n",
    "        info_list=info_list+results \n",
    "        logging.info('current length info_list is {}'.format(len(info_list)))\n",
    "        #__________________________________________________________ \n",
    "        #output restaurant results for each zipcode as a txt file text\n",
    "        #__________________________________________________________\n",
    "        output_name='google_place_API_{}_{}_{}_{}.txt'.format(key_word,int(i),city,state)\n",
    "        output_path=os.path.join(output_folder, output_name)\n",
    "        with open(output_path,'w') as output:\n",
    "            json.dump(results,output,indent=4)\n",
    "    print('Hey I am done')\n",
    "    return info_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part IV Take input and run functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what city?: Austin\n",
      "what state?: TX\n",
      "What kind of restaurant you would like to search?:taco\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yizhiyin/anaconda3/envs/PythonData/lib/python3.6/site-packages/ipykernel/__main__.py:33: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/Users/yizhiyin/anaconda3/envs/PythonData/lib/python3.6/site-packages/ipykernel/__main__.py:36: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/Users/yizhiyin/anaconda3/envs/PythonData/lib/python3.6/site-packages/ipykernel/__main__.py:38: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/Users/yizhiyin/anaconda3/envs/PythonData/lib/python3.6/site-packages/ipykernel/__main__.py:40: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/Users/yizhiyin/anaconda3/envs/PythonData/lib/python3.6/site-packages/ipykernel/__main__.py:42: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey I am done\n"
     ]
    }
   ],
   "source": [
    "#Creating a log\n",
    "logging.basicConfig(filename='Yummy_Data_Project.log',level=logging.DEBUG,format='%(asctime)s %(message)s')\n",
    "\n",
    "# user inputted variables and clean it for api call\n",
    "city = input(\"what city?: \")\n",
    "city = city.lower()\n",
    "city = ' '.join(word[0].upper() + word[1:] for word in city.split())\n",
    "state = input('what state?: ')\n",
    "state = state.upper()\n",
    "key_word=input('What kind of restaurant you would like to search?:')\n",
    "\n",
    "#get zip_list for the input city\n",
    "zip_list=get_zipcode(city,state)\n",
    "\n",
    "#get info_list from google places API\n",
    "\n",
    "info_list=get_google_place(key_word,zip_list,city,state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Part V Make Dataframe and save as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Extract useful information and create data_frame\n",
    "name=[]\n",
    "google_rating=[]\n",
    "google_price_level=[]\n",
    "formated_address=[]\n",
    "row_zipcode=[]\n",
    "google_id=[]\n",
    "lat=[]\n",
    "lng=[]\n",
    "for ele in info_list:\n",
    "    name.append(ele['name'])\n",
    "    google_id.append(ele['id'])\n",
    "    lat.append(ele[\"geometry\"]['location'][\"lat\"])\n",
    "    lng.append(ele[\"geometry\"]['location'][\"lng\"])\n",
    "    \n",
    "    #get address\n",
    "    if bool(ele.get(\"formatted_address\")):\n",
    "        formated_address.append(ele[\"formatted_address\"])\n",
    "    else:\n",
    "        formated_address.append(np.nan)\n",
    "    #not all restaurant has a rating\n",
    "    if bool(ele.get('rating')):\n",
    "        google_rating.append(ele['rating'])\n",
    "    #if no rating append np.nan(basically a flag pandas recognize as NaN)\n",
    "    else:\n",
    "        google_rating.append(np.nan)\n",
    "    #not all restaurant has a price_level\n",
    "    if bool(ele.get('price_level')):\n",
    "        google_price_level.append(ele['price_level'])\n",
    "    #if no price_level append np.nan\n",
    "    else:\n",
    "        google_price_level.append(np.nan)\n",
    "        \n",
    "    #use regular expression to extract zip code such as TX 78723\n",
    "    #in theory all zipcode should be extracted since the address is formatted \n",
    "    #but just add a codition to make sure\n",
    "    zip_store=re.findall('TX [0-9]{5}',ele['formatted_address'])\n",
    "#     print(ele['formatted_address'])\n",
    "#     print(zip_store)\n",
    "    if bool(zip_store):\n",
    "        row_zipcode.append(zip_store[0][3:])\n",
    "    #if there are np.nan in zipcode row, need to go back to mannuly fill in\n",
    "    else:\n",
    "        row_zipcode.append(np.nan)\n",
    "    \n",
    "        \n",
    "#Then make a dataframe\n",
    "df=pd.DataFrame({'Name':name,'Rating(Google)':google_rating,'Price_Level(Google)':google_price_level,\\\n",
    "                      'Zip':row_zipcode,'Address':formated_address,'ID(Google)':google_id,\\\n",
    "                'Latitude': lat, 'Longitude':lng},index=np.arange(0,len(name),1))\n",
    "#Drop duplicates\n",
    "df2 = df.drop_duplicates(subset='ID(Google)')\n",
    "#text search sometimes restaurants with address e.g 78721 street not its zipcode\n",
    "#this is not retrieved as zipcode need to drop those with np.nan on zipcode\n",
    "df3=df2.dropna(subset=['Zip'])\n",
    "logging.info('there are {} restaurants received and {} after dropping duplicates'.format(len(df),len(df2)))\n",
    "csv_name='{}_drop_duplicates_in_{}_{}.csv'.format(key_word,city,state)\n",
    "path_1=os.path.join('drop_duplicates', csv_name)\n",
    "\n",
    "#save unique results as csv in drop_duplicates folder\n",
    "df3.to_csv(path_1,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PythonData]",
   "language": "python",
   "name": "conda-env-PythonData-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
